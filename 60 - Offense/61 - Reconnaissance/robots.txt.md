robots.txt is a page that many websites have. It determines how web scrapers/crawlers are allowed to interact with that particular website.

For recon purposes, it will contain a list of other pages on that URL and may contain a list of pages that can't be accessed by search engines.